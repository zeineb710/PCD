from ultralytics import YOLO
import torch
import os
from pathlib import Path

def evaluate_model():
    """
    Script d'Ã©valuation dÃ©taillÃ©e pour un modÃ¨le YOLO dÃ©jÃ  entraÃ®nÃ©
    """
    
    # ====== CONFIGURATION ======
    model_path = r"C:\Users\MSI\Desktop\detection2\runs\detect\runs\yolov8_littering3\weights\best.pt"  # Chemin vers votre modÃ¨le
    data_yaml = r"C:\Users\MSI\Desktop\detection2\littering-detction.v4i.yolov8\data.yaml"
    output_dir = "evaluation_results"  # Dossier pour sauvegarder les rÃ©sultats
    
    # CrÃ©er le dossier de sortie
    os.makedirs(output_dir, exist_ok=True)
    
    # VÃ©rifier que le modÃ¨le existe
    if not os.path.exists(model_path):
        print(f"âŒ ModÃ¨le introuvable : {model_path}")
        print("ğŸ’¡ VÃ©rifiez le chemin ou entraÃ®nez d'abord votre modÃ¨le")
        return
    
    print("="*60)
    print("ğŸ” Ã‰VALUATION DU MODÃˆLE YOLO")
    print("="*60)
    print(f"ğŸ“‚ ModÃ¨le       : {model_path}")
    print(f"ğŸ“„ Dataset      : {data_yaml}")
    print(f"ğŸ’¾ RÃ©sultats    : {output_dir}/")
    print("="*60)
    
    # ====== CHARGER LE MODÃˆLE ======
    print("\nâ³ Chargement du modÃ¨le...")
    model = YOLO(model_path)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âœ… ModÃ¨le chargÃ© sur : {device}")
    
    # ====== VALIDATION ======
    print("\nğŸ”„ Lancement de la validation...")
    val_results = model.val(
        data=data_yaml,
        imgsz=640,
        batch=4,
        save_json=True,      # Sauvegarder les rÃ©sultats en JSON
        save_hybrid=False,
        plots=True,          # GÃ©nÃ©rer les graphiques
        project=output_dir,
        name="validation"
    )
    
    # ====== MÃ‰TRIQUES GLOBALES ======
    print("\n" + "="*60)
    print("ğŸ“Š MÃ‰TRIQUES GLOBALES")
    print("="*60)
    print(f"mAP50-95 (global)  : {val_results.box.map:.4f}   (0.0 - 1.0)")
    print(f"mAP50 (global)     : {val_results.box.map50:.4f}   (0.0 - 1.0)")
    print(f"mAP75 (global)     : {val_results.box.map75:.4f}   (0.0 - 1.0)")
    print(f"PrÃ©cision moyenne  : {val_results.box.mp:.4f}   (0.0 - 1.0)")
    print(f"Recall moyen       : {val_results.box.mr:.4f}   (0.0 - 1.0)")
    
    # ====== MÃ‰TRIQUES PAR CLASSE ======
    print("\n" + "="*60)
    print("ğŸ“‹ MÃ‰TRIQUES DÃ‰TAILLÃ‰ES PAR CLASSE")
    print("="*60)
    
    class_names = model.names
    maps_per_class = val_results.box.maps
    
    print(f"\n{'Classe':<15} {'mAP50-95':<12} {'Performance'}")
    print("-" * 60)
    
    for class_id, class_name in class_names.items():
        map_value = maps_per_class[class_id]
        
        # Ã‰valuation qualitative
        if map_value >= 0.8:
            perf = "ğŸŸ¢ Excellent"
        elif map_value >= 0.6:
            perf = "ğŸŸ¡ Bon"
        elif map_value >= 0.4:
            perf = "ğŸŸ  Moyen"
        else:
            perf = "ğŸ”´ Faible"
        
        print(f"{class_name:<15} {map_value:<12.4f} {perf}")
    
    # ====== MATRICE DE CONFUSION ======
    print("\n" + "="*60)
    print("ğŸ¯ MATRICE DE CONFUSION")
    print("="*60)
    
    if val_results.confusion_matrix is not None:
        confusion_path = os.path.join(output_dir, "validation", "confusion_matrix.png")
        val_results.confusion_matrix.plot(
            save_dir=os.path.join(output_dir, "validation"),
            names=list(class_names.values())
        )
        print(f"âœ… Matrice sauvegardÃ©e : {confusion_path}")
        
        # Afficher la matrice en texte
        matrix = val_results.confusion_matrix.matrix
        print(f"\nMatrice de confusion (format texte) :")
        print(f"{'':>10}", end="")
        for name in class_names.values():
            print(f"{name[:8]:>10}", end="")
        print("  Background")
        
        for i, name in enumerate(class_names.values()):
            print(f"{name[:10]:>10}", end="")
            for j in range(len(matrix[i])):
                print(f"{int(matrix[i][j]):>10}", end="")
            print()
    else:
        print("âš ï¸  Matrice de confusion non disponible")
    
    # ====== ANALYSE DES RÃ‰SULTATS ======
    print("\n" + "="*60)
    print("ğŸ” ANALYSE DES RÃ‰SULTATS")
    print("="*60)
    
    global_map = val_results.box.map
    precision = val_results.box.mp
    recall = val_results.box.mr
    
    # Diagnostic gÃ©nÃ©ral
    print("\nğŸ“Œ Diagnostic gÃ©nÃ©ral :")
    if global_map >= 0.7:
        print("   âœ… Excellente performance globale !")
    elif global_map >= 0.5:
        print("   âš¡ Performance correcte, amÃ©liorations possibles")
    else:
        print("   âš ï¸  Performance faible, rÃ©entraÃ®nement recommandÃ©")
    
    # Analyse prÃ©cision vs recall
    print("\nğŸ“Œ Analyse PrÃ©cision vs Recall :")
    if precision > 0.8 and recall < 0.6:
        print("   ğŸ¯ Haute prÃ©cision, faible recall")
        print("      â†’ Le modÃ¨le est conservateur (peu de dÃ©tections mais correctes)")
        print("      â†’ Solution : Baisser le seuil de confiance ou ajouter plus de donnÃ©es")
    elif precision < 0.6 and recall > 0.8:
        print("   ğŸ¯ Faible prÃ©cision, haut recall")
        print("      â†’ Le modÃ¨le dÃ©tecte beaucoup mais fait des erreurs")
        print("      â†’ Solution : Augmenter le seuil de confiance ou amÃ©liorer les donnÃ©es")
    elif precision > 0.7 and recall > 0.7:
        print("   âœ… Bon Ã©quilibre prÃ©cision/recall")
    else:
        print("   âš ï¸  PrÃ©cision et recall tous deux faibles")
        print("      â†’ Solution : Augmenter la quantitÃ©/qualitÃ© des donnÃ©es")
    
    # Analyse par classe
    print("\nğŸ“Œ Analyse par classe :")
    for class_id, class_name in class_names.items():
        map_value = maps_per_class[class_id]
        if map_value < 0.5:
            print(f"   âš ï¸  '{class_name}' a des performances faibles ({map_value:.2f})")
            print(f"      â†’ Ajoutez plus d'exemples de '{class_name}'")
            print(f"      â†’ VÃ©rifiez la qualitÃ© des annotations pour '{class_name}'")
    
    # ====== RECOMMANDATIONS ======
    print("\n" + "="*60)
    print("ğŸ’¡ RECOMMANDATIONS D'AMÃ‰LIORATION")
    print("="*60)
    
    recommendations = []
    
    if global_map < 0.5:
        recommendations.append("1. ğŸ“¸ Augmentez votre dataset (min 1000 images par classe)")
        recommendations.append("2. ğŸ” VÃ©rifiez la qualitÃ© de vos annotations")
        recommendations.append("3. ğŸ”„ Appliquez plus d'augmentation de donnÃ©es")
    
    if global_map < 0.7:
        recommendations.append("4. â±ï¸  Augmentez le nombre d'epochs (100-200)")
        recommendations.append("5. ğŸš€ Essayez un modÃ¨le plus grand (yolov8m ou yolov8l)")
    
    if any(maps_per_class[i] < 0.4 for i in range(len(maps_per_class))):
        recommendations.append("6. ğŸ¯ Collectez plus d'exemples pour les classes faibles")
        recommendations.append("7. âš–ï¸  Ã‰quilibrez votre dataset entre les classes")
    
    if len(recommendations) == 0:
        print("   ğŸ‰ Votre modÃ¨le performe bien !")
        print("   ğŸ“ˆ Pour aller plus loin :")
        print("      - Testez sur de nouvelles donnÃ©es rÃ©elles")
        print("      - Optimisez pour l'infÃ©rence (export ONNX/TensorRT)")
    else:
        for rec in recommendations:
            print(f"   {rec}")
    
    # ====== FICHIERS GÃ‰NÃ‰RÃ‰S ======
    print("\n" + "="*60)
    print("ğŸ“ FICHIERS GÃ‰NÃ‰RÃ‰S")
    print("="*60)
    print(f"Dossier : {output_dir}/validation/")
    print("   - confusion_matrix.png      : Matrice de confusion")
    print("   - confusion_matrix_normalized.png : Matrice normalisÃ©e")
    print("   - val_batch*_pred.jpg       : PrÃ©dictions visualisÃ©es")
    print("   - val_batch*_labels.jpg     : Labels rÃ©els")
    
    print("\nâœ… Ã‰valuation terminÃ©e !")
    print(f"ğŸ“‚ Consultez les rÃ©sultats dans : {output_dir}/")

if __name__ == "__main__":
    evaluate_model()